mlp:
  hidden_dim: 768
  expand_size: 4
attn:
  num_heads: 6
  num_kv_heads: 6
  hidden_dim: ${mlp.hidden_dim}
gpt:
  layers: 12
  rope_dtype: bf16
  seq_len: 1024
  unembedding_lr: .004
  embedding_lr: .3
  matrix_lr: .02
tokenizer:
  vocab_size: 32768
meta:
  train_dtype: bf16
  use_wandb: False
  wandb_log_steps: 100
  device_batch_size: 32
  resume_from_step: -1
  adam_beta1: .8
  adam_beta2: .95
  weight_decay: .2
  grad_accum_steps: 1
  warmup_ratio: 0.0
  warmdown_ratio: 0.4
  final_lr_frac: 0.0
  max_steps: 1000
  sample_every: 100
  eval_every: 100
  core_metric_every: 100
  core_metric_max_per_task: 500
