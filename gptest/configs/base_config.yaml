mlp:
  hidden_dim: 768
  expand_size: 4
attn:
  num_heads: 6
  num_kv_heads: 6
  hidden_dim: ${mlp.hidden_dim}
gpt:
  layers: 12
  rope_dtype: bf16
  seq_len: 1024
  unembedding_lr: .004
  embedding_lr: .3
  matrix_lr: .02
tokenizer:
  vocab_size: 32768
meta:
  # core
  train_dtype: bf16
  device_batch_size: 32
  resume_from_step: -1
  # optimization / LR
  adam_beta1: .8
  adam_beta2: .95
  weight_decay: .2
  grad_accum_steps: 1
  warmup_ratio: 0.0
  warmdown_ratio: 0.4
  final_lr_frac: 0.0
  # eval
  sample_every: 10
  eval_every: 10
  core_metric_every: 10
  core_metric_max_per_task: 10
  # step count determination
  max_steps: 1000
  data_param_ratio: 21.0  # empirical from chinchilla
  target_flops: -1
  chinchilla: params # enum one of ("params", "flops", "none")
  # logging
  use_wandb: False
  wandb_log_steps: 100